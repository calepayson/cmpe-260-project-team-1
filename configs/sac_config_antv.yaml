seed: 42
algo_name: "sac"

env:
  id: "Ant-v5"
  n_envs: 4    
  env_kwargs: 
    exclude_current_positions_from_observation: true 
    
vec_normalize:
  enabled: true
  norm_obs: true
  norm_reward: true        
  clip_obs: 10.0
  gamma: 0.99

algo:
  policy: "MlpPolicy"     

  # Learning - Ant is harder and needs more careful tuning
  learning_rate: 0.0001    
  buffer_size: 1000000     
  learning_starts: 10000   
  batch_size: 256          
  tau: 0.005               
  gamma: 0.99              

  # Update cadence - Ant benefits from more gradual updates
  train_freq: [1, "step"]  
  gradient_steps: 1        
  
  ent_coef: "auto"         
  target_entropy: "auto"   

  # Networks - Ant needs larger networks due to complexity
  policy_kwargs:
    net_arch:
      pi: [256, 256]       
      qf: [256, 256]       
    activation_fn: "relu" 
    log_std_init: -3         
  verbose: 1

training:
  total_timesteps: 1000000  
  tensorboard_log: "./logs/sac_ant_tb/"

  checkpoint:
    enabled: true
    save_freq_timesteps: 100000  
    save_path: "./models/sac_ant/"
    name_prefix: "sac_ant"

  resume: false
  resume_model_path: "./models/sac_ant/final_model.zip"
  resume_vecnormalize_path: "./models/sac_ant/vec_normalize_stats.pkl"

evaluation:
  enabled: true
  n_eval_episodes: 20
  deterministic: true     
  render: false
  eval_freq_timesteps: 2500 
  eval_env_kwargs: 
    exclude_current_positions_from_observation: true