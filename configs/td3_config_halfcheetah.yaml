seed: 42
algo_name: "td3"

env:
  id: "HalfCheetah-v5"
  n_envs: 8                # 8-16 parallel environments is efficient for HalfCheetah
  env_kwargs: {}           # No special kwargs needed for standard Mujoco

vec_normalize:
  enabled: true
  norm_obs: true
  norm_reward: true        # CRITICAL: Normalize rewards for HalfCheetah (unlike sparse fetch)
  clip_obs: 10.0
  gamma: 0.99

algo:
  policy: "MlpPolicy"      # Use MlpPolicy for Box observation space (not MultiInputPolicy)

  # Exploration
  action_noise_sigma: 0.1  # Standard Gaussian noise magnitude for locomotion

  # Learning
  learning_rate: 0.001     # 1e-3 (Aggressive) or 3e-4 (Standard). TD3 handles 1e-3 well here.
  buffer_size: 1000000
  learning_starts: 25000   # Longer warmup than Fetch
  batch_size: 256
  tau: 0.005
  gamma: 0.99

  # Update cadence
  # For HalfCheetah, updating every step (or equivalent) is standard
  train_freq: [1, "step"] 
  gradient_steps: -1       # -1 means "same as number of steps collected in episode"

  # TD3 specifics
  policy_delay: 2
  target_policy_noise: 0.2
  target_noise_clip: 0.5
  
  # Networks
  policy_kwargs:
    net_arch:
      pi: [256, 256]
      qf: [256, 256]
    activation_fn: "relu"

  verbose: 1

training:
  total_timesteps: 1000000
  tensorboard_log: "./logs/td3_halfcheetah_tb/"

  checkpoint:
    enabled: true
    save_freq_timesteps: 50000
    save_path: "./models/td3_halfcheetah/"
    name_prefix: "td3_hc"

  resume: false
  resume_model_path: "./models/td3_halfcheetah/final_model.zip"
  resume_vecnormalize_path: "./models/td3_halfcheetah/vec_normalize_stats.pkl"

evaluation:
  enabled: true
  n_eval_episodes: 20
  deterministic: true
  render: false
  eval_freq_timesteps: 1250
  eval_env_kwargs: {}