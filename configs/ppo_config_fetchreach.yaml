# config.yaml
# Production-ready PPO configuration for FetchReach-v4 with SB3

seed: 42
algo_name: "ppo"


env:
  id: "FetchReach-v4"      
  n_envs: 4                
  env_kwargs:
    max_episode_steps: 50
    reward_type: "dense"    


vec_normalize:
  enabled: true
  norm_obs: true
  norm_reward: false         
  clip_obs: 10.0


algo:
  policy: "MultiInputPolicy"

  n_steps: 1024
  batch_size: 256
  n_epochs: 4

  # RL hyperparameters
  gamma: 0.98
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.01
  vf_coef: 0.5
  learning_rate: 1.0e-4
  max_grad_norm: 0.5

  # Network architecture
  policy_kwargs:
    net_arch:
      pi: [256, 256]
      vf: [256, 256]
    activation_fn: "tanh"    

  verbose: 1

training:
  total_timesteps: 100000

  # Logging
  tensorboard_log: "./logs/ppo_fetch_tb/"
  csv_log: "./logs/ppo_fetch_csv/progress.csv"

  # Checkpointing
  checkpoint:
    enabled: true
    save_path: "./models/ppo_fetch/"
    save_freq_timesteps: 50000
    name_prefix: "ppo_fetch_reach"

  # Resume logic
  resume: false
  resume_model_path: "./models/ppo_fetch/ppo_fetch_reach_1000000_steps.zip"
  resume_vecnormalize_path: "./models/ppo_fetch/vec_normalize_stats.pkl"


evaluation:
  enabled: true
  n_eval_episodes: 20
  deterministic: true
  render: false
  eval_freq_timesteps: 10000   
  eval_env_kwargs:
    max_episode_steps: 50
    reward_type : "dense"

curriculum:
  enabled: false
