# config.yaml
# Production-ready SAC configuration for FetchReach-v4 with SB3

# ----------------------------------------------------------------------
# Global Settings
# ----------------------------------------------------------------------
seed: 42
algo_name: "sac"

# ----------------------------------------------------------------------
# Environment Configuration
# ----------------------------------------------------------------------
env:
  id: "FetchReach-v4"        # Requires gymnasium_robotics
  n_envs: 4                 # Number of parallel envs for training
  env_kwargs:
    max_episode_steps: 50
    reward_type: "dense"     # Dense reward speeds up training

# ----------------------------------------------------------------------
# Vector Normalization
# ----------------------------------------------------------------------
vec_normalize:
  enabled: true
  norm_obs: true
  norm_reward: false         # False for sparse/dense rewards
  clip_obs: 10.0

# ----------------------------------------------------------------------
# Algorithm (SAC) Configuration
# ----------------------------------------------------------------------
algo:
  policy: "MultiInputPolicy"

  # SAC-specific hyperparameters
  learning_rate: 3.0e-4 #from paper
  buffer_size: 1000000 #from paper
  batch_size: 256 #from paper
  tau: 0.005 #from paper
  gamma: 0.99 #taken from paper

  # Training schedule
  train_freq: 1       # gradient update every step
  gradient_steps: 1
  learning_starts: 1000

  # Entropy (automatic entropy recommended)
  ent_coef: "auto"

  # HER Replay buffer
  replay_buffer_class: "HerReplayBuffer"
  replay_buffer_kwargs:
    n_sampled_goal: 4
    goal_selection_strategy: "future" #Best for FetchReach

  # Network architecture
  policy_kwargs:
    net_arch:
      pi: [256, 256]
      qf: [256, 256]
    activation_fn: "ReLu"    # More stable for control tasks

  verbose: 1

# ----------------------------------------------------------------------
# Training Configuration
# ----------------------------------------------------------------------
training:
  total_timesteps: 500000

  # Logging
  tensorboard_log: "./logs/sac_fetch_tb/"
  csv_log: "./logs/sac_fetch_csv/progress.csv"

  # Checkpointing
  checkpoint:
    enabled: true
    save_path: "./models/sac_fetch/"
    save_freq_timesteps: 50000
    name_prefix: "sac_fetch_reach"

  # Resume logic
  resume: false
  resume_model_path: "./models/sac_fetch/sac_fetch_reach_1000000_steps.zip"
  resume_vecnormalize_path: "./models/sac_fetch/vec_normalize_stats.pkl"

# ----------------------------------------------------------------------
# Evaluation Configuration
# ----------------------------------------------------------------------
evaluation:
  enabled: true
  n_eval_episodes: 20
  deterministic: true
  render: false
  eval_freq_timesteps: 10000   
  eval_env_kwargs:
    max_episode_steps: 100

# ----------------------------------------------------------------------
# Curriculum Learning (Optional)
# ----------------------------------------------------------------------
curriculum:
  enabled: false
